## 1. 🎯 이 작업의 종류와 목표

- [ ] 🧪 **실험 (exp/)**: 새로운 가설 검증 또는 알고리즘 시도 (WIP)
- [ ] ✨ **기능 (feat/)**: 확정된 기능 개발 (WIP)
- [ ] 🐛 **버그 수정 (fix/)**: `main` 브랜치의 버그 수정 (보통 바로 Merge)

### 📌 목표 (Goal)
- (예: 기존 모델의 수렴 속도를 20% 향상시키기 위한 AdamW 옵티마이저 도입 및 검증)
- (예: 사용자 피드백을 반영하여 데이터 로딩 시 발생하는 인덱스 에러 해결)


## 2. 📝 진행 상황 (To Do List)

### A. 작업 단계
- [ ] 1. `AdamW` 옵티마이저 클래스 정의
- [ ] 2. `config.yaml`에 옵티마이저 설정 파라미터 추가
- [ ] 3. 단일 GPU 환경에서 학습 및 Loss 커브 확인
- [ ] 4. (필요 시) 새로운 지표(Metric) 계산 로직 추가
- [ ] 5. 최종 성능 벤치마크 테스트 진행


### B. 코드 정리 및 최종 확인
- [ ] 6. 불필요한 디버깅 코드(`print` 등) 제거
- [ ] 7. 관련 설정 파일(e.g., `README.md`) 업데이트 확인
- [ ] 8. **(중요!)** 이 실험과 관련 없는 버그는 `stash` 후 `fix/` 브랜치로 처리했나요? (코드 오염 방지)


## 3. 📊 [🧪 실험 PR일 때 사용] 주요 결과 및 기록

### 3.1. 현재 가설
- (예: AdamW는 SGD보다 더 적은 Epoch으로 수렴할 것이다.)

### 3.2. 중간 결과 (Optional)
- **[2025/11/20]** 초기 Learning Rate 0.001로 테스트 시, 초반 Loss가 튀는 현상 발견. `git log` 확인 후 L.R. 0.0005로 수정 예정.

### 3.3. 최종 결과 (Merge 시 필수)
* **결론:** [성공 / 실패 / 보류] (최종 상태 선택)
* **지표 비교:** * 기존 (SGD): F1 Score 0.85
    * 실험 (AdamW): F1 Score 0.89 (목표치 0.88 달성)
* **결과 이미지/로그:** (성능 그래프, 콘솔 로그 스크린샷 등)
  

## 4. 🔗 (선택) 참고 자료 및 논의 사항